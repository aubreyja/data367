---
title: "Logistic Regression with James Harden"
author: Notes adapted from Jason Spector, Jericho Lawson, and Aaron Ekstrom
format:
  html: default
  pdf: default
fontsize: 11pt
---

## Overview
Having a game-changing player in the NBA can affect how successful his team is throughout the season. James Harden is no exception. While he was with the Rockets, he has made a name for himself through lots and lots of scoring (even if plenty of those points came from the free throw line). With that said, how well does James Harden need to do to ensure that the Rockets get a win? We will use logistic regression to answer this question. 

![](https://thespun.com/.image/ar_1:1,c_fill,cs_srgb,fl_progressive,q_auto:good,w_1200/MTgzMTI4NTIwODI1Nzc1NDU2/brooklyn-nets-v-houston-rockets.jpg){width=250}


## Logistic Regression Review

Logistic regression uses the machinery of linear regression, but adapts it for binary responses. The main limitation of linear regression is that the responses can be any real number. However, with binary responses, we are often interested in probabilities, which range between 0 and 1. 

The way we adapt linear regression for logistic regression is by applying a _logit_ transform. We model the _log-odds_ with linear regression:
$$
\log\left(\frac{p}{1-p}\right) = \hat{\beta_0} + \hat{\beta}_1 x.
$$
After some mathematical manipulation, we can rewrite it in terms of probability: 
$$
p = \frac{1}{1 + \exp\left(-\left(\hat{\beta}_0 + \hat{\beta}_1 x\right) \right)}
= \frac{\exp\left(\hat{\beta}_0 + \hat{\beta}_1 x \right)}{1 + \exp\left(\hat{\beta}_0 + \hat{\beta}_1 x \right)}.
$$



### Exploratory Data Analysis
In this demonstration, we are going to examine the relationship between the number of points scored by James Harden and whether the Rockets would win or lose. The data comes from 545 games played by James Harden and the Houston Rockets from October 31, 2012 to April 9, 2019.

```{python}
# Import Libraries
import pandas as pd
import numpy as np
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt

# Load Data
harden = pd.read_csv("data/harden_revised.csv", index_col=0)
```

The first thing you should ever do when you get new data is to do some _exploratory data analysis_. You should orient yourself with what the data offers. What variables are considered? Does the data require additional data cleaning? 


```{python}
# Exploratory Data Analysis
harden.head()
```

**Numerical Encoding.** Today, we'll keep things simple and only look at the relationship between winning games (`Result`) and the number of points James Harden scored in the game (`PTS`). 

`Result` takes on character values of either `W` (win) or `L` (loss). Characters are harder to work with, so it customary to use a numerical encoding. In this dataset, we are provided that numerical encoding via the `Dummy` variable. It takes on a value of `0` for losses and `1` for wins. 

**Plotting Results**. Now that we've narrowed the scope of our problem to identifying the relationship between scoring and winning games, we're going to start by plotting the data. 

How valuable is James Harden to the Houston Rockets? As the figure shows below, the more points Harden scores, the more likely the Rockets are of winning games. However, aside from this surface-level observation, it's not clear just how much impact he has on the team.

```{python}
# Plot Points Scored by Harden vs. Wins
plt.figure()
plt.scatter(harden["PTS"], harden["Dummy"], alpha=0.2)
  # The alpha parameter sets the opacity of the points, making the density
  # of the points easier to see.
plt.xlabel("Points Scored")
plt.ylabel("Win (Win = 1, Loss = 0)")
plt.title("Rockets Wins based on Points Scored by James Harden")
plt.show()
```

## Logistic Regression
As we've discussed previously, linear regression is a powerful method that is useful in parsing out the impact of various predictors on some response. Unfortunately, linear regression is not fit when the responses take on binary values. Instead, we should be using _logistic regression_.

To use linear regression, we use code like this: `smf.ols("Y ~ X", data=df).fit()`. Logistic regression is not so different. Instead of calling `smf.ols`, we call `smf.logit`.

```{python}
# Fit Logistic Regression Model
log_fit = smf.logit("Dummy ~ PTS", data=harden).fit()
```

**Predictions**. Now that we've fit the model, we can use it to make predictions. We see that if James Harden scores 20 points, there's a 55% chance the Rockets win. Similarly, if he scores 40 points, there's a 75% chance the Rockets win. 

```{python}
# Make dataframe with same input columns as the model
pts = pd.DataFrame({"PTS": [20, 40, 50]})

# Get Predictions
log_fit.predict(pts)
```


This is much easier to see in a graph. 

```{python}
# Plot Wins and Losses
plt.figure()
plt.scatter(harden["PTS"], harden["Dummy"], alpha=0.2)
plt.xlabel("Points Scored")
plt.ylabel("Win (Win = 1, Loss = 0)")
plt.axhline(0.5, color="gray", linestyle="--") # Show 50% cutoff
plt.title("Rockets win probability vs points scored by James Harden")

# Get Predicted Probabilities of Winning, At Various Values of `PTS`
pts = np.linspace(harden["PTS"].min(), harden["PTS"].max(), 200)
pred_df = pd.DataFrame({"PTS": pts}) # log_fit requires a data frame input
p_hat = log_fit.predict(pred_df)     # get estimated probabilities

# Add Logistic Regression Line 
plt.plot(pts, p_hat, color="black")
plt.ylim(-0.05, 1.05)
plt.show()
```

**Interpretation.** Once again, we see that as the more points Harden scores, the more likely the Rockets are to win, but it is much clearer now. If Harden scores over ~15 points, the Rockets are more likely to win than lose. 

### Evaluation
A _confusion matrix_ is a useful tool to identify how well our model identifies true successes and failures. In our example, it tells us whether Harden's scoring is a good predictor of whether or not the Rockets win. 

```{python}
# Import Confusion Matrix
from sklearn.metrics import (confusion_matrix, accuracy_score)

# True labels
y_true = harden["Dummy"]

# Predicted probabilities
y_prob = log_fit.predict()

# Convert to class labels (threshold = 0.5)
y_pred = (y_prob >= 0.5).astype(int)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
pd.DataFrame(
    cm,
    index=["Actual Loss (0)", "Actual Win (1)"],
    columns=["Predicted Loss (0)", "Predicted Win (1)"]
)
```

**True Positives**. The model correctly predicted 339 wins. 

**True Negatives**. The model correctly predicted 24 losses.

**False Positives**. The model falsely predicted 169 wins.

**False Negatives**. The model falsely predicted 13 losses.

It looks like our model has a tendency to predict wins and is poor at predicting losses. Specifically, our model is able to predict ~96% of winning games, but only 12% of losing games. This results in a high false positive rate. 

---
