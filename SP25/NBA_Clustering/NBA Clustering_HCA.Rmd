---
output:
  pdf_document: default
  html_document: default
  always_allow_html: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, warning = FALSE, error = FALSE, message = FALSE)
```




## Clustering

Clustering is a form of unsupervised learning that aims to group similar data points together into clusters, thereby revealing hidden patterns or structures within the dataset.

When working with higher-dimensional data, defining the closeness or similarity between two points becomes essential. The most common method for measuring this closeness is through **Euclidean Distance**. It quantifies the straight-line distance between two points in Euclidean space. Given two points \( P = (x_1, y_1) \) and \( Q = (x_2, y_2) \), the Euclidean distance between them is calculated using the formula:

\[ d(P, Q) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \]

This distance metric is frequently employed in clustering algorithms to assess the dissimilarity or similarity between data points.

Another challenge arises when dealing with features that have different magnitude levels. In such cases, features with larger scales may disproportionately influence distance calculations, potentially leading to biased results. To address this issue, we employ **Scaling**, a process that transforms data to a common scale, ensuring that all features contribute equally to the analysis. Common scaling techniques include standardization (Z-score normalization) and Min-Max scaling. Scaling ensures that each feature has equal importance in the clustering process, irrespective of its original scale.

**Clustering Algorithms:**

1. **Hierarchical Clustering (HCA)**:
   
   - HCA is a approach to clustering where each data point starts in its cluster, and pairs of clusters are merged or split based on their similarity.
   - The algorithm builds a dendrogram, a tree-like structure that represents the hierarchical relationships between clusters.
   - HCA does not require the number of clusters to be specified beforehand, making it flexible.
   - Two main types of hierarchical clustering are agglomerative (bottom-up) and divisive (top-down).

## Clustering in Sports

Every basketball loving fan at some point in their life will have the discussion, which of two players is better. As a child, my friends and I would often argue over Jason Kidd or Steve Nash, D-Wade or Lebron, or Kobe or Allen Iverson (AI). To this day, I have friends who say the 2001-2002 season AI was a better scorer than Kobe and I continue to respond with it wasn't as close as you think. AI scored 31.4 points per game in the 01-02 season and Kobe scored 25.2 points per a game. Six points is a significant difference in points per game but once you take into account that AI played 43.7 minutes per game and Kobe played 38.3 minutes, simple math tells you that Kobe scored .66 points per minute and AI scored .71 points per minute. That means if both played forty minute games at their season scoring rate Kobe would have 26.4 points and AI would have 28.4 points. A measly two-point difference. Add in the fact that Kobe shot 46.9 percent from the floor and AI shot 39.8 percent from the floor, the statement becomes said with a little less confidence. Are there players currently in the NBA who once put to a fairer opportunity standard are performing better than their per game statistics and are the coaches noticing? In this experiment we are solely looking at offensive ability as defensive statistics are not very good at completely capturing defensive ability. 


Start by loading the data!

```{r}
per_36_2019_all <- read.csv("per_36_min_2019.csv")

per_36_2018_all <- read.csv("per_36_min_2018.csv")

per_game_2019_all <- read.csv("per_game_2019.csv")

per_game_2018_all <- read.csv("per_game_2018.csv")

```

Take a subset of the data, we are looking at simpler statistics.
```{r}
per_36_2019 <- per_36_2019_all[ ,c("player","age","G","GS","MP","FG_per", "X3P_per", "X2P_per"  ,"FT_per", "TRB", "AST", "STL", "BLK", "TOV", "PTS_per_36", "team") ]

per_36_2018 <- per_36_2018_all[ ,c("player","age","G","GS","MP","FG_per", "X3P_per", "X2P_per"  ,"FT_per", "TRB", "AST", "STL", "BLK", "TOV", "PTS_per_36", "team") ]

per_game_2019 <- per_game_2019_all[ ,c("player","age","G","GS","MP","FG_per", "X3P_per", "X2P_per"  ,"FT_per", "TRB", "AST", "STL", "BLK", "TOV", "PTS_per_G", "team") ]

per_game_2018 <- per_game_2018_all[ ,c("player","age","G","GS","MP","FG_per", "X3P_per", "X2P_per"  ,"FT_per", "TRB", "AST", "STL", "BLK", "TOV", "PTS_per_G", "team") ]

```

Description of the data:
Player: Player's Name 

Age: Avg age of team

G: Games played

GS: Games Started

MP: Minutes played

FG_per: Field Goal percentage

3P_per: 3-point percentage (3P/3PA)

2P_per: 2-point percentage (2P/2PA)

FT_per: Free Throw percentage (FT/FTA)

TRB: Total Rebounds (ORB + DRB)

AST: Assists

STL: Steals

BLK: Blocks

TOV: Turnovers

PTS_per_G OR PTS_per_G: Points per game or points per 36 minutes

Team: Team abbreviation



Now with analyzable data the method continued as followed:

1) We began by looking at the per-36-minute data to search for outliers.
2) Then a cluster analysis of offensive ability. Offensive ability can often be boiled down to scoring ability and ability to make your teammates score. Thus, we looked at clusters using PT_per_G and AST. We acknowledge that more variables could be used in creating clusters but, by using only two it allows for a 2-D interpretable graph that coaches can use to take action.
3) We fit hierarchical agglomerative cluster analysis (HCA) model to determine the number of clusters recommended by critical values on per_game basis for 2017-18 season. 
4) Once we determined the number of clusters we used a kmean clustering model with the HCA centroids as our starting point instead of random points.
5) We repeated steps 2 and 3 for a per-36-minute period. 
6) We continued by applying the player's assigned per-game cluster to the players per-36-minute data. 
7) By doing this we were able to analyze which players when remaining constant with their per game pace would be considered much better players when given the minutes opportunity.




```{r}
library(ggplot2)
library(plotly)
library(gridExtra)
library(leaps)
library(psych)
library(NbClust)
library(sqldf)
```


We analyze the distribution of the per-36-minute data to see if we have any outliers. The problem with the per-36-minute data is that a player with a small sample size in minutes could have scored a high volume quickly.
```{r}
pairs.panels(x = per_36_2018[, c('G', 'PTS_per_36', 'AST')],
             ellipses = TRUE,	
             lm=FALSE, 	
             smooth = TRUE,	
             show.points = TRUE, 	
             density = TRUE,
             hist.col = "dodgerblue",	
             breaks = 10		
)

```

We quickly noticed this was the case as there was a player who would score 108 points in 36 minutes. This player scored 3 points in 1 minute of total play time for a season. Because of situations like these we removed the bottom 10 percent of players by minutes per game. This means in the 2017-18 season, we removed anyone who when playing in a game played less than 5.24 minutes. Then we also removed players who played in less than 10 games. We did this to try to avoid players who might be only playing in 'garbage time'. 

```{r}
rm_ind <- which(per_game_2018$MP > quantile(per_game_2018$MP, .10) | per_game_2018$G > 10)

per_game_2018 <- per_game_2018[rm_ind, ]
per_36_2018 <- per_36_2018[rm_ind, ]


```



Next we standardized AST and PTS_per_G by dividing each value by the range and fit the hierarchical agglomerative cluster analysis (HCA) model to determine how many clusters we should be using.

```{r}
#scale
per_game_2018$PTS_per_G_std <- per_game_2018$PTS_per_G/range(per_game_2018$PTS_per_G)[2]
per_game_2018$AST_std <- per_game_2018$AST/range(per_game_2018$AST)[2]
per_game_2018$TRB_std <- per_game_2018$TRB/range(per_game_2018$TRB)[2]


```

```{r}
hca.fit <- NbClust(data = per_game_2018[,c('PTS_per_G_std', 'AST_std')],
				distance = "euclidean", 
				method = "ward.D",
				index="all")
```

8 indices suggested 2 and 5 suggested 3. We will fit our model with three clusters as it is the most practical when it comes to the NBA. With three clusters we hope to see an elite player, a intermediate player, and reserves. For our model's clustering method we used ward.D and Euclidean distance for determining distance between points. 
 
 

```{r}
#this code chunk gives us our distance matrix
d.matrix <- dist(x= per_game_2018[,c('PTS_per_G_std', 'AST_std')], method="euclidean")
#d.matrix
```



```{r}
#This chunk fits our model!
hclust.fit <- hclust(d=d.matrix, method="ward.D")
#hclust.fit
```



```{r}
#this chunk assigns clusters!
per_game_2018$mem3 <- cutree(hclust.fit, k=3)
```

Once HCA assigned a cluster to our players we used those cluster's centroid's as a starting point for our K-means clustering. We started with the three cluster model. The centroids were as follows:

```{r}
c1mean <- c(mean(per_game_2018$PTS_per_G_std[per_game_2018$mem3==1]),
			mean(per_game_2018$AST_std[per_game_2018$mem3==1])
			)

c2mean <- c(mean(per_game_2018$PTS_per_G_std[per_game_2018$mem3==2]),
			mean(per_game_2018$AST_std[per_game_2018$mem3==2])
			)

c3mean <- c(mean(per_game_2018$PTS_per_G_std[per_game_2018$mem3==3]),
			mean(per_game_2018$AST_std[per_game_2018$mem3==3])
			)


per_game_2018_centroids <- rbind(c1mean, c2mean, c3mean)
per_game_2018_centroids
```

## Group Activity for Today

1. Select any two different variables from the data `per_game_2019`. Explain why you choose those variables.
2. Explain the scaling method you are using and why.
3. Report the number of clusters.
4. Provide the centroids of the clusters.


